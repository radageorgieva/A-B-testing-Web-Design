# A-B-testing-Web-Design
A/B tests are very important when it comes to determining optimisations which thus leads to maximizing revenues in digital businesses. A/B experiments are rather simplistic and are mostly leveraged for design questions; thus, this technique works quite well in an online setting as parameters of interest can easily be changed, allowing us to conduct trials in order to justify business decisions. 

A/B testing provides the ability to test web design changes and sense users' reaction to those changes before deploying them. For this report we will create a website for a small Food & Beverages business called “Drops of Heaven”, which originated in Brazil and has now relocated to London, where it offers a range of baked goods all based on the typical Brazilian fudgy sauce “Brigadeiro”. Apart from the typical fudgy Brigadeiro balls, Drops of Heaven offers Brownies and the emblematic Volcano cake. Orders are placed through direct messages on their Instagram channel. Hence, the listing of products on our website will include buttons redirecting the user to this channel.   

The website was created via Weebly, and can be found using the following link:  http://dropsofheavenbrigadeiros.weebly.com/. It consists of a Home Page, a Product Page and a Blog Page. The Home Page introduces the products, containing buttons linking to the Products page. The Product page provides assortment, pricing information and contains “Order” buttons that take the visitor to the direct messages of the Instagram Account of the business. The Blog Page introduces the story behind the goods, and allows visitors to interact and leave comments. 

The question we want to test for our website is what is the optimal color for our Call-to-Action button that leads to a higher conversion rate. We use the predefined Google Optimize objective Pageviews as a conversion goal. This will help us answer which CTA button color leads to a higher average number of pageview events per user in the experiment. 
As the end goal of each conversion rate optimization is to translate into revenue, we will test the button that is most closely related to our business’ bottom line. In this case, we use the button with text “Enter the store” at the button of the Homepage. This button takes the visitor to the Product listing page, that in turns directs the visitors to the Instagram Sales Channel where orders can be placed.

To gether data, we track the clickstream by implementing a tag management system to collect analytics on a webpage. It consists of a couple of Google Analytics Tags, which we then manually add to the header and footer of each page of the website. 

The first two tags are used to interact with the website without interfering with the server optimization. They enable us to view and analyze clickstream data real time in Google Analytics: 

The Google Optimize Tag is also placed in the header of the html code to link the A/B test we will construct to the website’s homepage. In this way, we ensure we can track the pageviews and other conversion metrics for the original website (controlled), as well as all other variants we would like to test . Each variant includes one design change, which we believe influences conversion rates. Then we track the latter in terms of pageviews, that is, the average number of pageview events per user in the experiment. This is the best inbuilt metric we could track on Google Optimize given that our website is not of an e-commerce essence. After enough users have interacted with each variant we can look at the readily available A/B test results on Google Optimize and can download the raw data to run our own Hypothesis Analysis.

Crazy Egg is an alternative to Google Optimize which enables us not only to run A/B tests, but also to look at heatmaps to help us understand how impressions vary across the position on the landing page. It also enables us to define custom goals such as the clicks of a specific button.

The information we would like to extract from the test is the experiment pageviews and the calculated pageviews per user on each version of the landing page.

We will take these already available metrics into account but will also run our own One- and Two-Sided Null Hypothesis Analyses. In the Two-Sided test we will set the alternative hypothesis to be that the original landing page pageview per user mean is higher than the one of the variant landing page. As we have constructed the experiment thinking that the blue color of the button would increase the number of pageviews, our goal would be to fail to reject the null hypothesis showing that we have high certainty that its mean is smaller in the original version.
In order to obtain a sufficient sample, we used word-of-mouth strategy (sending the link to our friends and families), sharing the link in our cohort’s group, as well as posting it on one of our Facebook profiles. On the 15th day we had 204 experiment impressions, with 58 experiment clients in the original landing page and 62 in the variant one.

The results of the Two-Sample T-test with Unequal Variance prove that the performance of the two version of the landing page are significantly different, as the p-value is close to zero and we reject the null Hypothesis stating that the two distributions have the same means: 

As we want to understand the direction in which the performance differs, i.e, which variant has better performance in driving pageviews per client, we perform the same Hypothesis but this time stating as an alternative hypothesis that the Original land page has higher average number of pageviews per client compared to the Variant1 landing page:

Since the p-value is close to zero, we reject the null hypothesis stating that the Original landing page has worse or equal performance as Variant1. 
